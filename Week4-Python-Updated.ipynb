{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing in Python\n",
    "\n",
    "Let's look at doing some preprocessing using pandas and scikit-learn in Python.\n",
    "\n",
    "First of all, we need to import the packages we want to use: numpy, pandas and scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```conda install scikit-learn```\n",
    "\n",
    "or\n",
    "\n",
    "```pip install scikit-learn```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "# For graphing/visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening the data file\n",
    "Now we need to open the abalone file. We do this using pandas. You can search for these in the pandas help yourself, but the functions of interest are:\n",
    "- `pandas.read_csv` and `pandas.to_csv` to read and write CSV files.\n",
    "- `pandas.read_excel` and `pandas.to_excel` to read and write MS Excel files\n",
    "\n",
    "We're going to open the abalone file. Note: you will need to edit the code to ensure that it points to where you have downloaded the `abalone-small.xls` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data = pd.read_excel(\"./abalone-small.xls\")\n",
    "abalone_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abalone Data Description\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Given is the attribute name, attribute type, the measurement unit and a brief description. The number of rings is the value to predict: either as a continuous value or as a classification problem.\n",
    "\n",
    "Name / Data Type / Measurement Unit / Description\n",
    "-----------------------------\n",
    "\n",
    "| Name            | Data Type    | Measurement Unit    | Description        |\n",
    "|:----------------|:------------|:-------|:-----------------------------|\n",
    "| Sex            | nominal    | --    | M, F, and I (infant)        |\n",
    "| Length         | continuous | mm    | Longest shell measurement   |\n",
    "| Diameter       | continuous | mm    | perpendicular to length     |\n",
    "| Height         | continuous | mm    | with meat in shell          |\n",
    "| Whole weight   | continuous | grams | whole abalone               |\n",
    "| Shucked weight | continuous | grams | weight of meat              |\n",
    "| Viscera weight | continuous | grams | gut weight (after bleeding) |\n",
    "| Shell weight   | continuous | grams | after being dried           |\n",
    "| Rings          | integer    | --    | +1.5 gives the age in years |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a quick look at the data\n",
    "You can of course just display the variable or use `.head()`, `.tail()` or `.sample()` to see the top or bottom of the dataset. We can also quickly check the `head` **and** `tail` with the `.concat()` function to join two tables together along the row axis.\n",
    "\n",
    "### Concatenate\n",
    ">In formal language theory and computer programming, string concatenation is the operation of joining character strings end-to-end. For example, the concatenation of \"snow\" and \"ball\" is \"snowball\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat([abalone_data.head(), abalone_data.tail()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape the abalone data\n",
    "abalone_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the column names with `.columns` and the row indices with `.index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access the columns of the data\n",
    "abalone_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "First, let's look at scaling (normalising) the data. We do this with the following:\n",
    "\n",
    "- `sklearn.preprocessing.StandardScaler` for Z-score normalisation\n",
    "- `sklearn.preprocessing.MinMaxScaler` for min-max normalisation\n",
    "\n",
    "For each, you `.fit` to work out the scaler setting (e.g., the mean and variance) then `.transform` when you want to use it. That means you can do scale different DataFrames in the same way. If you want to do both together, then you can just use `.fit_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column \"Sex\"\n",
    "abalone_subdata = abalone_data.drop(['Sex'],axis=1)\n",
    "abalone_subdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Performing a Standard scaler transform of the Abalone dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot\n",
    "std_scaler = StandardScaler()\n",
    "data = std_scaler.fit_transform(abalone_subdata)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = pd.DataFrame(data,columns=['Index','Length', 'Diameter', 'Height', 'Gross mass',\n",
    "       'Meat mass', 'Gut mass', 'Shell mass', 'Age'])\n",
    "\n",
    "# summarize\n",
    "dataset.describe()\n",
    "\n",
    "# Check mean & Standard dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "\n",
    "dataset.hist(figsize=(12,12))\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a minmax scaler transform of the Abalone dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "min_max = MinMaxScaler()\n",
    "data1 = min_max.fit_transform(abalone_subdata)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset1 = pd.DataFrame(data1,columns=['Index','Length', 'Diameter', 'Height', 'Gross mass',\n",
    "       'Meat mass', 'Gut mass', 'Shell mass', 'Age'])\n",
    "\n",
    "# summarize\n",
    "print(dataset1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "dataset1.hist(figsize=(12,12))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a minmax scaler transform of the Abalone dataset with range (-3,3)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "\n",
    "min_max = MinMaxScaler(feature_range=(-3,3))\n",
    "data3 = min_max.fit_transform(abalone_subdata[\"Height\"].values.reshape(-1, 1))\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset3 = pd.DataFrame(data3,columns=['Height'])\n",
    "\n",
    "# summarize\n",
    "print(dataset3.describe())\n",
    "\n",
    "# histograms of the variables\n",
    "dataset3.hist()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other preprocessing classess of interest\n",
    "\n",
    "You might also be interested in \n",
    "`sklearn.preprocessing.OneHotEncoder` and `sklearn.preprocessing.LabelBinarizer` (for the target column). These do a one hot encoding of the data and labels respectively. Note: these are quite different in older versions of scikit-learn.\n",
    "\n",
    "There is also `sklearn.preprocessing.KBinsDiscretizer` which does binning. Again, this only exists in newer versions of scikit-learn, so you may need to write yourself if it isn't there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot = OneHotEncoder(dtype=int,sparse=False)\n",
    "amalone_sex=onehot.fit_transform(abalone_data[['Sex']])\n",
    "amalone_sex= pd.DataFrame(amalone_sex,columns=['Female', 'Male', 'Infant'])\n",
    "amalone_sex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like categorical data can be encoded, numerical features can be ‘decoded’ into categorical features. \n",
    "The two most common ways to do this are discretization and binarization.\n",
    "\n",
    "Discretization:also known as quantization or binning, divides a continuous feature into a pre-specified number of categories (bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "disc = KBinsDiscretizer(n_bins=4, encode='ordinal')\n",
    "abalone_subdata1 = disc.fit_transform(abalone_subdata)\n",
    "abalone_subdata2= pd.DataFrame(abalone_subdata1,columns=['Index','Length', 'Diameter', 'Height', 'Gross mass',\n",
    "       'Meat mass', 'Gut mass', 'Shell mass', 'Age'])\n",
    "abalone_subdata2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load the iris data used in previous weeks to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create a \"data dictionary\" describing the dataset's attributes (as above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) perform standard normalisation on a copy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Perform min-max normalisation on a copy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) On the either the min-maxed or normalised dataset, perform one-hot encoding on the \"species\" attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
